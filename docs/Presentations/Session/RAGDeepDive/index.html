<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>The Future of Information Retrieval</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/carvana.css">
    <link rel="stylesheet" href="css/headers.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section data-transition="slide-in slide-out" data-background='img/19ae31f8-078b-4bd3-8411-0222b6a09c25.jpg'>
<a id="ee062bcb-426b-49d3-8e8a-2d0e005245d0">&nbsp;</a>
<!-- Layout:ImageRight -->
<!-- ContentItem:8ac9d5ff-1136-4d99-8448-de49bd005889 -->
<!-- ContentItem:f28e9518-3a64-4a5c-85ad-73a750ce0a62 -->

<table><tr><td style="vertical-align:top;">
<h1 id="rag-deep-dive">RAG Deep-Dive</h1>
<hr />
<h3 id="barry-s.stahl">Barry S. Stahl</h3>
<h3 id="solution-architect-developer">Solution Architect &amp; Developer</h3>
<h3 id="bsstahlcognitiveinheritance.com"><a href="https://fosstodon.org/@Bsstahl">@bsstahl@cognitiveinheritance.com</a></h3>
<h3 id="httpscognitiveinheritance.com"><a href="https://cognitiveinheritance.com">https://CognitiveInheritance.com</a></h3>

</td>
<td width="60%"><img alt="Transparent Half Width Image.png" src="img/f28e9518-3a64-4a5c-85ad-73a750ce0a62.png" /></td>
</tr></table></section>

<section data-transition="slide-in slide-out">
<a id="636059f9-aa9d-4444-b4ec-dd7f62badd98"><h1>Favorite Physicists & Mathematicians</h1></a>
<!-- Layout:MultiColumn -->
<!-- ContentItem:30f7e9d2-b154-4895-bed7-2765f7bdc6fe -->
<!-- ContentItem:8a83be06-76e9-4028-9630-c6a7de92ae99 -->

<table><tr><td style="vertical-align:top;">
<h4 id="favorite-physicists">Favorite Physicists</h4>
<ol>
<li>Harold &quot;Hal&quot; Stahl</li>
<li>Carl Sagan</li>
<li>Richard Feynman</li>
<li>Marie Curie</li>
<li>Nikola Tesla</li>
<li>Albert Einstein</li>
<li>Neil Degrasse Tyson</li>
<li>Niels Bohr</li>
<li>Galileo Galilei</li>
<li>Michael Faraday</li>
</ol>
<p><em>Other notables</em>: Stephen Hawking, Edwin Hubble</p>

</td>
<td style="vertical-align:top;">
<h4 id="favorite-mathematicians">Favorite Mathematicians</h4>
<ol>
<li>Ada Lovelace</li>
<li>Alan Turing</li>
<li>Johannes Kepler</li>
<li>Rene Descartes</li>
<li>Isaac Newton</li>
<li>Leonardo Fibonacci</li>
<li>George Boole</li>
<li>Blaise Pascal</li>
<li>Johann Gauss</li>
<li>Grace Hopper</li>
</ol>
<p><em>Other notables</em>: Daphne Koller, Grady Booch, Evelyn Berezin</p>

</td>
</tr></table></section>

<section data-transition="slide-in slide-out">

<a id="23528a73-7bc1-4e39-9f2f-c8b9e2cff982"><h1>Some OSS Projects I Run</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:f8cbcb81-15f7-48e0-bc29-b348732f6f05 -->
<ol>
<li><a href="https://github.com/bsstahl/liquidvictor">Liquid Victor</a> : Media tracking and aggregation [used to assemble this presentation]</li>
<li><a href="https://github.com/bsstahl/pptail">Prehensile Pony-Tail</a> : A static site generator built in c#</li>
<li><a href="https://github.com/bsstahl/testhelperextensions">TestHelperExtensions</a> : A set of extension methods helpful when building unit tests</li>
<li><a href="https://github.com/bsstahl/conferencescheduler">Conference Scheduler</a> : A conference schedule optimizer</li>
<li><a href="https://github.com/bsstahl/intentbot">IntentBot</a> : A microservices framework for creating conversational bots on top of Bot Framework</li>
<li><a href="https://github.com/bsstahl/liquidnun">LiquidNun</a> : Library of abstractions and implementations for loosely-coupled applications</li>
<li><a href="https://github.com/bsstahl/toastmastersagenda">Toastmasters Agenda</a> : A c# library and website for generating agenda's for Toastmasters meetings</li>
<li><a href="https://github.com/bsstahl/PDM">ProtoBuf Data Mapper</a> : A c# library for mapping and transforming ProtoBuf messages</li>
</ol>

</section>


<section data-transition="slide-in slide-out">

<a id="3129a405-82a7-432c-ae55-6f9d2335ab17"><h1>http://GiveCamp.org</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:2ca346de-d62c-4c6b-834d-7ccab290a6ab -->
<img alt="GiveCamp.png" src="img/2ca346de-d62c-4c6b-834d-7ccab290a6ab.png" />
</section>


<section data-transition="slide-in slide-out">
<aside class="notes"><p>July 25, 2022 was my 100th public talk. Thank you for allowing me to participate in your community over these past 13 years!</p>
</aside>
<a id="39c6410c-3913-410b-abab-984014a15d84">&nbsp;</a>
<!-- Layout:FullPage -->
<!-- ContentItem:a042b106-4bab-4403-b5b6-0d7b414ef789 -->
<img alt="bss-100-achievement-unlocked-1024x250.png" src="img/a042b106-4bab-4403-b5b6-0d7b414ef789.png" />
</section>


<section data-transition="slide-in slide-out" data-background='img/4b6b21f6-04f6-4b44-b48b-a620c295ee4d.png'>
<aside class="notes"><p>Completions is a legacy capability that has been superceded by ChatCompletions.</p>
</aside>
<a id="35a7a96d-3a7b-4cac-a5d7-79b6bc8c6455"><h1>The OpenAI API</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:5e187c38-77c2-46a8-907b-ae16a5c956aa -->
<ul>
<li>Chat Completions
<ul>
<li>ChatGPT gets most of its power here</li>
</ul>
</li>
<li>Embeddings
<ul>
<li>Enable additional features that can be used with Chat Completions</li>
<li>Especially useful operationally</li>
</ul>
</li>
</ul>

</section>


<section data-transition="slide-in slide-out">
<aside class="notes"><p>We tend to focus on productivity and analytics when we talk about GPT type models. I'd like to look at some of the operational uses. Embeddings are a big part of that.</p>
</aside>
<a id="b3b2e119-9289-418e-8ad6-0f365013e1b9"><h1>Questions to be Answered</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:f024f688-b495-4fb5-a3dc-bae44d30a9fc -->
<ul>
<li>What are embeddings?</li>
<li>What do they represent?</li>
<li>How do we compare/contrast them?</li>
<li>How can we use them operationally?</li>
</ul>

</section>


<section data-transition="slide-in slide-out" data-background='img/4b6b21f6-04f6-4b44-b48b-a620c295ee4d.png'>
<a id="5d979917-37b8-4337-a076-be890a1d2d61"><h1>Embeddings</h1></a>
<!-- Layout:FullPageFragments -->
<!-- ContentItem:d4a17eef-5a35-4595-82bd-43b11c9362d9 -->
<!-- ContentItem:212cfe49-195f-4554-9941-3a05bd43f89b -->
<table border="0" width="100%">
<aside class="notes"><p>There are many other embedding models, some are optimized for different purposes. You should consider what model might work best for your use-case. This presentation uses the same model used by GPT-3 and GPT-4.</p>
</aside>
<tr><td>
<ul class="fragment">
<li>A point in multi-dimensional space</li>
<li>Mathematical representation of a word or phrase</li>
<li>Encode both <strong>semantic</strong> and <strong>contextual</strong> information</li>
</ul>

</td></tr>
<tr><td>
<ul class="fragment">
<li>Model: <code>text-embedding-ada-002</code></li>
<li>Vectors normalized to unit length</li>
<li>Use 1536 dimensions</li>
</ul>

</td></tr>
</table></section>


<section data-transition="slide-in slide-out">
<aside class="notes"><p>It is not possible for us to reason on space greater than 3D without doing some form of projection.</p>
<p>Image Source: WikiMedia</p>
</aside>
<a id="affc18de-26dd-4bed-acbe-99beddf497f4"><h1>3-D Space Projected into 2-D</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:8738d954-63a2-40f5-aa5e-deeb431d5f52 -->
<img alt="Necker_cube_with_background.png" src="img/8738d954-63a2-40f5-aa5e-deeb431d5f52.png" />
</section>


<section data-transition="slide-in none-out">
<aside class="notes"><p>I used a T-SNE (T-distributed Stochastic Neighbor Embedding) transformation to perform this dimensionality reduction</p>
</aside>
<a id="9ec34ef5-afab-43e9-abf1-010e746b7af4">&nbsp;</a>
<!-- Layout:FullPage -->
<!-- ContentItem:2313fe54-9904-4e1c-b5de-3c29c297fdc7 -->
<img alt="Ram - Just Statements.png" src="img/2313fe54-9904-4e1c-b5de-3c29c297fdc7.png" />
</section>


<section data-transition="none-in none-out">
<aside class="notes"><p>I used a T-SNE (T-distributed Stochastic Neighbor Embedding) transformation to perform this dimensionality reduction</p>
</aside>
<a id="50da6b31-0139-4b4c-bdc6-7f7eedef6044">&nbsp;</a>
<!-- Layout:FullPage -->
<!-- ContentItem:ba598d38-6e47-4a26-8065-3487bfd3f51a -->
<img alt="Ram - With Terms.png" src="img/ba598d38-6e47-4a26-8065-3487bfd3f51a.png" />
</section>


<section data-transition="none-in slide-out">
<aside class="notes"><p>Ram - With Clusters.png</p>
</aside>
<a id="3714c934-4902-45a1-9948-3b31c7f02bba">&nbsp;</a>
<!-- Layout:FullPage -->
<!-- ContentItem:43926256-864d-4a0a-aad0-b7ea6fa5b74e -->
<img alt="Ram - With Clusters.png" src="img/43926256-864d-4a0a-aad0-b7ea6fa5b74e.png" />
</section>


<section data-transition="slide-in slide-out">
<a id="72e0090e-a40f-4410-91e0-919b3ed09004"><h1>Cosine Similarity &amp; Distance</h1></a>
<!-- Layout:MultiColumn -->
<!-- ContentItem:89b66910-a26e-4116-b41d-3ada3f74c92b -->
<!-- ContentItem:d44bda2e-06bd-4064-9c06-12d564a6f5cf -->
<aside class="notes"><p>Since GPT vectors are normalized to unit length, Cosine distance is proportional to Euclidean distance, and will result in the same relative rankings.</p>
</aside>
<table><tr><td style="vertical-align:top;">
<blockquote>
<p>Relate vectors based on the angle between them</p>
</blockquote>
<ul>
<li><p><em>Cosine Similarity</em> ranges from -1 to 1, where:</p>
<ul>
<li>+1 indicates that the vectors represent similar semantics &amp; context</li>
<li>0 indicates that the vectors are orthogonal (no similarity)</li>
<li>-1 indicates that the vectors have opposing semantics &amp; context</li>
</ul>
</li>
<li><p><em>Cosine Distance</em> is defined as <strong>1 - cosine similarity</strong> where:</p>
<ul>
<li>0 = Synonymous</li>
<li>1 = Orthogonal</li>
<li>2 = Antonymous</li>
</ul>
</li>
</ul>

</td>
<td style="vertical-align:top;">
<img alt="Cosine Unit Circle - Enhanced.jpg" src="img/d44bda2e-06bd-4064-9c06-12d564a6f5cf.jpg" />
</td>
</tr></table></section>

<section data-transition="slide-in slide-out">

<a id="0e2a1080-2694-447d-85ec-30f6863051de"><h1>Cosine Distance</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:1c9aab67-70df-45a0-b05d-4aae3c3a20c5 -->
<img alt="Cosine Distance 989x600.png" src="img/1c9aab67-70df-45a0-b05d-4aae3c3a20c5.png" />
</section>


<section data-transition="slide-in slide-out">
<aside class="notes"><p>I added additional context here: i.e. &quot;Sheep&quot; =&gt; &quot;Bighorn Sheep&quot;, to make the distances more visible.</p>
</aside>
<a id="952e53c0-3883-490e-a43f-c3f86853fc08"><h1>Cosine Distance</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:b079c831-8d8c-425b-8e49-c8674d0b3ec6 -->
<img alt="Angles2.svg" src="img/b079c831-8d8c-425b-8e49-c8674d0b3ec6.svg" />
</section>


<section data-transition="slide-in slide-out">
<a id="c504d2aa-f3e6-4956-b06c-96ba4a551dfc"><h1>Clustering</h1></a>
<!-- Layout:MultiColumn -->
<!-- ContentItem:81fec6ba-5786-4258-b8ac-a742fe19af01 -->
<!-- ContentItem:53a6cce5-95de-4e12-8035-bf68503c1bbe -->
<aside class="notes"><p>Clusters contains items more similar to each other than to those in other clusters</p>
<ul>
<li>Key to use: Experiment</li>
</ul>
<p>Source Image: <a href="https://www.bing.com/images/search?view=detailV2&amp;ccid=vNng%2foOs&amp;id=2652F88E274436CB6C8CC623A778D7B9F2475FC9&amp;thid=OIP.vNng_oOsNRHKrlh3pjSAyAHaDw&amp;mediaurl=https%3a%2f%2fi.stack.imgur.com%2fcIDB3.png&amp;cdnurl=https%3a%2f%2fth.bing.com%2fth%2fid%2fR.bcd9e0fe83ac3511caae5877a63480c8%3frik%3dyV9H8rnXeKcjxg%26pid%3dImgRaw%26r%3d0&amp;exph=517&amp;expw=1017&amp;q=k-means+clustering&amp;simid=608026322933788461&amp;FORM=IRPRST&amp;ck=8E4BDA3E627011B4CADA2B68FB818490&amp;selectedIndex=0&amp;qft=+filterui%3alicenseType-Any&amp;ajaxhist=0&amp;ajaxserp=0">https://www.bing.com/images/search?view=detailV2&amp;ccid=vNng%2foOs&amp;id=2652F88E274436CB6C8CC623A778D7B9F2475FC9&amp;thid=OIP.vNng_oOsNRHKrlh3pjSAyAHaDw&amp;mediaurl=https%3a%2f%2fi.stack.imgur.com%2fcIDB3.png&amp;cdnurl=https%3a%2f%2fth.bing.com%2fth%2fid%2fR.bcd9e0fe83ac3511caae5877a63480c8%3frik%3dyV9H8rnXeKcjxg%26pid%3dImgRaw%26r%3d0&amp;exph=517&amp;expw=1017&amp;q=k-means+clustering&amp;simid=608026322933788461&amp;FORM=IRPRST&amp;ck=8E4BDA3E627011B4CADA2B68FB818490&amp;selectedIndex=0&amp;qft=+filterui%3alicenseType-Any&amp;ajaxhist=0&amp;ajaxserp=0</a></p>
</aside>
<table><tr><td style="vertical-align:top;">
<ul>
<li>Unsupervised machine learning technique</li>
<li>Clusters form around centroids (the geometric center of the cluster)</li>
<li>Data points are grouped (clustered) based on their similarity
<ul>
<li>Minimize the error (distance from centroid)</li>
</ul>
</li>
<li>Embeddings cluster with others of similar semantic and contextual meaning</li>
<li>Advantages
<ul>
<li>No need to define a distance threshold</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>Quality is use-case dependent</li>
<li>Requires the number of clusters to be specified</li>
</ul>
</li>
</ul>

</td>
<td style="vertical-align:top;">
<img alt="k-means results.png" src="img/53a6cce5-95de-4e12-8035-bf68503c1bbe.png" />
</td>
</tr></table></section>

<section data-transition="slide-in slide-out">

<a id="b1b27a07-42ee-481c-92f4-6791f7204828"><h1>Embedding Distance</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:fcee17b4-8b97-487e-9644-0c45033c1ff3 -->
<table>
<thead>
<tr>
<th>Feature</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Synonym</td>
<td>&quot;Happy&quot; is closer to &quot;Joyful&quot; than to &quot;Sad&quot;</td>
</tr>
<tr>
<td>Language</td>
<td>&quot;The Queen&quot; is very close to &quot;La Reina&quot;</td>
</tr>
<tr>
<td>Idiom</td>
<td>&quot;He kicked the bucket&quot; is closer to &quot;He died&quot; than to &quot;He kicked the ball&quot;</td>
</tr>
<tr>
<td>Sarcasm</td>
<td>&quot;Well, look who's on time&quot; is closer to &quot;Actually Late&quot; than &quot;Actually Early&quot;</td>
</tr>
<tr>
<td>Homonym</td>
<td>&quot;Bark&quot; (dog sound) is closer to &quot;Howl&quot; than to &quot;Bark&quot; (tree layer)</td>
</tr>
<tr>
<td>Collocation</td>
<td>&quot;Fast food&quot; is closer to &quot;Junk food&quot; than to &quot;Fast car&quot;</td>
</tr>
<tr>
<td>Proverb</td>
<td>&quot;The early bird catches the worm&quot; is closer to &quot;Success comes to those who prepare well and put in effort&quot; than to &quot;A bird in the hand is worth two in the bush&quot;</td>
</tr>
<tr>
<td>Metaphor</td>
<td>&quot;Time is money&quot; is closer to &quot;Don't waste your time&quot; than to &quot;Time flies&quot;</td>
</tr>
<tr>
<td>Simile</td>
<td>&quot;He is as brave as a lion&quot; is closer to &quot;He is very courageous&quot; than to &quot;He is a lion&quot;</td>
</tr>
</tbody>
</table>

</section>


<section data-transition="slide-in slide-out">
<a id="295f37ae-5979-40ae-a5d9-a22bd58ef30a"><h1>Vector Databases</h1></a>
<!-- Layout:MultiColumn -->
<!-- ContentItem:fd61eed6-fd5e-4ce2-b9c9-f4c06f5287ff -->
<!-- ContentItem:7a67ae3f-567b-4099-b9ea-a2c2fb484d7f -->
<aside class="notes"><p>The key to operational use of Embeddings is some form of Vector data store, whether that is persistent or in-memory</p>
</aside>
<table><tr><td style="vertical-align:top;">
<ul>
<li>Designed to store/retrieve high-dimensional vectors</li>
<li>Values are retrieved using similarity searches</li>
<li>Leverage data-structures such as K-D Trees</li>
<li>Examples
<ul>
<li>Azure Cognitive Search</li>
<li>Redis</li>
<li>Qdrant</li>
<li>Pinecone</li>
<li>Chroma</li>
</ul>
</li>
</ul>

</td>
<td style="vertical-align:top;">
<img alt="VectorDB-650x650.png" src="img/7a67ae3f-567b-4099-b9ea-a2c2fb484d7f.png" />
</td>
</tr></table></section>

<section data-transition="slide-in slide-out">
<aside class="notes"><p>If we are not augmenting the model with our own data, we are just hoping the model happens to be trained on the right things for our use-case.</p>
</aside>
<a id="f8c3912e-08e3-46e9-b5b9-92f492e5780b"><h1>Retrieval Augmented Generation (RAG)</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:7d502157-fe4b-4891-9ddb-90d231368958 -->
<ul>
<li>Combines the benefits of retrieval-based and generative models
<ul>
<li>Identify and retrieve relevant information</li>
<li>Agument context of the generative models</li>
<li>Generative responses based on the augmented context</li>
</ul>
</li>
<li>Potential uses include
<ul>
<li>Explore large sets of documentation conversationally</li>
<li>Generate recommendations and insights based on retrieved relevant information</li>
<li>Summarization of articles in light of known relevant information</li>
</ul>
</li>
</ul>

</section>


<section data-transition="slide-in slide-out">

<a id="f1a2a6fe-439d-435e-b9e8-20af0aa13d69"><h1>Beary - The Beary Barry Bot</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:0538ef66-2942-4c64-8b4e-7f5fb652be13 -->
<img alt="Beary_600x600.png" src="img/0538ef66-2942-4c64-8b4e-7f5fb652be13.png" />
</section>


<section data-transition="slide-in slide-out">

<a id="6ed666b8-ccc0-4ce0-b035-1e5532a9ea77"><h1>Beary Flow</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:c19314dc-adde-4e88-9f01-c781f26d19b6 -->
<img alt="Beary Demo - Flowchart - Horizontal Flow.png" src="img/c19314dc-adde-4e88-9f01-c781f26d19b6.png" />
</section>


<section data-transition="slide-in slide-out">

<a id="9fa65486-faeb-4be1-8973-46698fa008b5"><h1>Beary Embeddings Json Snippet</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:9e0c1a7c-5a7a-4734-83ae-93233fb99341 -->
<img alt="Beary Embeddings Json Snippet.png" src="img/9e0c1a7c-5a7a-4734-83ae-93233fb99341.png" />
</section>


<section data-transition="slide-in slide-out">
<aside class="notes"><p>The ethics of AI are complex, and you should explore them. The simplest rule is: &quot;If you can't build it ethically with an AI model, don't build it with an AI model&quot;</p>
<p>Image Source: <a href="https://www.bing.com/images/search?view=detailV2&amp;ccid=SF5NW%2F2E&amp;id=867710BF172B3480BE22F1F1FE19397AE0A6FE8A&amp;thid=OIP.SF5NW_2E8OBt2DbDD1pomgHaDE&amp;mediaurl=https%3A%2F%2Fth.bing.com%2Fth%2Fid%2FR.485e4d5bfd84f0e06dd836c30f5a689a%3Frik%3Div6m4Ho5Gf7x8Q%26riu%3Dhttp%253a%252f%252fmarcogomes.com%252fblog%252fwp-content%252fuploads%252f2010%252f09%252fherebedragons.jpg%26ehk%3DFB%252fMQocKYDXUvxHKhrvHWg9Meu0PUBJWlSnfk%252bjgSp4%253d%26risl%3D%26pid%3DImgRaw%26r%3D0%26sres%3D1%26sresct%3D1&amp;exph=224&amp;expw=540&amp;q=here+there+be+dragons&amp;simid=608053647533357231&amp;form=IRPRST&amp;ck=2AD9AF425E791ED0BDF2696C5EB0557E&amp;selectedindex=7&amp;qft=+filterui%3alicenseType-Any&amp;ajaxhist=0&amp;ajaxserp=0&amp;vt=2&amp;sim=15,16&amp;ajaxhist=0&amp;ajaxserp=0">https://www.bing.com/images/search?view=detailV2&amp;ccid=SF5NW%2F2E&amp;id=867710BF172B3480BE22F1F1FE19397AE0A6FE8A&amp;thid=OIP.SF5NW_2E8OBt2DbDD1pomgHaDE&amp;mediaurl=https%3A%2F%2Fth.bing.com%2Fth%2Fid%2FR.485e4d5bfd84f0e06dd836c30f5a689a%3Frik%3Div6m4Ho5Gf7x8Q%26riu%3Dhttp%253a%252f%252fmarcogomes.com%252fblog%252fwp-content%252fuploads%252f2010%252f09%252fherebedragons.jpg%26ehk%3DFB%252fMQocKYDXUvxHKhrvHWg9Meu0PUBJWlSnfk%252bjgSp4%253d%26risl%3D%26pid%3DImgRaw%26r%3D0%26sres%3D1%26sresct%3D1&amp;exph=224&amp;expw=540&amp;q=here+there+be+dragons&amp;simid=608053647533357231&amp;form=IRPRST&amp;ck=2AD9AF425E791ED0BDF2696C5EB0557E&amp;selectedindex=7&amp;qft=+filterui%3alicenseType-Any&amp;ajaxhist=0&amp;ajaxserp=0&amp;vt=2&amp;sim=15,16&amp;ajaxhist=0&amp;ajaxserp=0</a></p>
</aside>
<a id="c28997ab-1cec-407e-b069-06efd5dc5a7f"><h1>Using LLM Output Has Dangers</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:d950c2c7-f2fe-4af9-b210-18d33c40661a -->
<img alt="herebedragons.jpg" src="img/d950c2c7-f2fe-4af9-b210-18d33c40661a.jpg" />
</section>


<section data-transition="slide-in slide-out">
<a id="302ab20a-f006-4521-8c30-2dbc46d99ed6"><h1>Model Answers May Be</h1></a>
<!-- Layout:MultiColumn -->
<!-- ContentItem:540f4702-43af-4a34-915e-6f8ec49ca672 -->
<!-- ContentItem:3b97dc48-fe4a-4f30-8139-3aa93949bd6b -->
<aside class="notes"><p>Whether using GPT directly or semantic search with embeddings.
Old image: 59bf56c7-2e3d-4a25-b78e-ade863990440</p>
</aside>
<table><tr><td style="vertical-align:top;">
<ul>
<li>Incomplete</li>
<li>Poorly phrased</li>
<li>Outright wrong</li>
</ul>

</td>
<td style="vertical-align:top;">
<img alt="No Takesies backsies.png" src="img/3b97dc48-fe4a-4f30-8139-3aa93949bd6b.png" />
</td>
</tr></table></section>

<section data-transition="slide-in slide-out">
<a id="08f50362-8bf7-48a2-a61a-0980f21d9a19"><h1>The model is biased</h1></a>
<!-- Layout:MultiColumn -->
<!-- ContentItem:0b06fef2-cc47-4acb-963e-4aa8a1a2aaeb -->
<!-- ContentItem:0c946a74-2029-4651-9eee-a2645dba9317 -->

<table><tr><td style="vertical-align:top;">
<ul>
<li>Not handling the bias makes bias a feature of your app</li>
<li>Prevent all predictable biases</li>
<li>Watch for unpredictable biases</li>
</ul>

</td>
<td style="vertical-align:top;">
<img alt="bias logo - large.jpg" src="img/0c946a74-2029-4651-9eee-a2645dba9317.jpg" />
</td>
</tr></table></section>

<section data-transition="slide-in slide-out">
<a id="c77448f6-5c86-44aa-96a4-e5d960c222ef"><h1>Embeddings are Reversable</h1></a>
<!-- Layout:MultiColumn -->
<!-- ContentItem:cccf95ad-420d-4027-ba9d-8e33f762eb6b -->
<!-- ContentItem:18980ad3-d112-4fd7-9221-717689b8ae12 -->
<aside class="notes"><p>Article from Linus Lee of Notion linked on Resources slide</p>
</aside>
<table><tr><td style="vertical-align:top;">
<ul>
<li>Researchers have had success in reversing embeddings
<ul>
<li>Using distance-measurements against a large Vector DB</li>
<li>Using models trained to predict the text from the embedding</li>
</ul>
</li>
<li>Embeddings can be thought-of like a hash
<ul>
<li>Data is obscured, but not encrypted</li>
</ul>
</li>
<li>Do not expect embeddings alone to protect PII
<ul>
<li>Encrypt or tokenize all PII before embedding</li>
</ul>
</li>
</ul>

</td>
<td style="vertical-align:top;">
<img alt="Simpleicons_Interface_unlocked-padlock - Red 600x600.png" src="img/18980ad3-d112-4fd7-9221-717689b8ae12.png" />
</td>
</tr></table></section>

<section data-transition="slide-in slide-out">
<aside class="notes"><p>I worry that situations where it is safe to use these models are more rare than people realize or are willing to accept. That said, we can't let the only people building using AI models be those whose only goal is to separate us from our wealth.</p>
</aside>
<a id="944ae71d-a8b3-4de4-9a9e-ffeef6e07fcd"><h1>When Should AI be Used?</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:0b8f654e-2dc5-41c1-b195-f0495b79d62e -->
<ul>
<li>When all possible biases have been accounted for</li>
<li>When all sensitive data has been removed, tokenized or encrypted</li>
<li>When the stochastic nature of responses has been accounted for
<ul>
<li>A wrong answer is no worse than no answer</li>
<li>Outputs have been fully constrained</li>
<li>A human is in-the-loop to fix the inevitable errors</li>
</ul>
</li>
</ul>

</section>


<section data-transition="slide-in slide-out">

<a id="ac8bc960-baa4-41e1-a289-3d2352f7cf96"><h1>What Are Embeddings?</h1></a>
<!-- Layout:FullPage -->
<!-- ContentItem:41ce545a-ffe3-4ec6-b4c6-43c4516c63b1 -->
<ul>
<li>Arrays of 1536 floating-point values</li>
<li>Structured numeric data that represents unstructured text</li>
<li>Representations of the semantics and context of the source text</li>
<li>Vectors that support standard mathematical operations</li>
</ul>

</section>


<section data-transition="slide-in slide-out">
<a id="8c8e5769-8a07-4a92-a315-1a02d79c9bfb"><h1>Resources</h1></a>
<!-- Layout:MultiColumn -->
<!-- ContentItem:97720f70-277d-44b0-b113-274ffa0db619 -->

<table><tr><td style="vertical-align:top;">
<ul>
<li>This Presentation - <a href="https://RAGDeepDive.azurewebsites.net/">Web</a> | <a href="?print-pdf">PDF</a></li>
<li>These Demos - <a href="https://github.com/bsstahl/AIDemos/tree/master/Embeddings">Code</a> | <a href="https://github.com/bsstahl/AIDemos/wiki/Embeddings">Docs</a></li>
<li><a href="https://cognitiveinheritance.com/Posts/depth-of-gpt-embeddings.html">The Depth of GPT Embeddings</a></li>
<li><a href="https://cognitiveinheritance.com/Posts/Programmers-Take-Responsibility-for-Your-AIe28099s-Output.html">Programmers -- Take Responsibility for Your AIâ€™s Output</a></li>
<li><a href="https://thesephist.notion.site/text-embedding-ada-002-embedding-reconstructions-2ea55ae9525d49dfa9d16213474e0895">Experiments in Reconstructing Text from Embeddings</a></li>
<li><a href="https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI">Azure OpenAI Client</a></li>
<li><a href="https://github.com/microsoft/semantic-kernel">Semantic Kernel</a></li>
<li><a href="https://www.youtube.com/watch?v=UnURElCzGc0">Carl Sagan - Reasoning on Higher Dimensions (YouTube)</a></li>
</ul>

</td>
</tr></table></section>


        </div>
        <div class="footer-left">The Future of Information Retrieval</div>
        <div class="footer-right">Barry S. Stahl - Mastodon:@bsstahl@cognitiveinheritance.com - Blog:http://www.cognitiveinheritance.com</div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
        // More info about config & dependencies:
        // - https://github.com/hakimel/reveal.js#configuration
        // - https://github.com/hakimel/reveal.js#dependencies
        Reveal.initialize({
			width: 1920,
			height: 1080,
            transition: 'slide',
            dependencies: [
                { src: 'plugin/markdown/marked.js' },
                { src: 'plugin/markdown/markdown.js' },
                { src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
                { src: 'plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } }
            ]
        });
    </script>
</body>
</html>
