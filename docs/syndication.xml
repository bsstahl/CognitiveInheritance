<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cognitive Inheritance</title>
    <description>The Application Development Experiences of an Enterprise Developer</description>
    <link>http://www.cognitiveinheritance.com/</link>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>Prehensile Pony Tail 1.0</generator>
    <language>en-US</language>
    <atom:link href="https://cognitiveinheritance.com/syndication.xml" rel="self" type="application/rss+xml" />	
    <item>
  <title>Continuing a Conversation on LLMs</title>
  <description>&lt;p&gt;This post is the continuation of a conversation from Mastodon. The thread begins &lt;a href=&quot;https://fosstodon.org/@arthurdoler@mastodon.sandwich.net/110177834073037647&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I understand and share your concerns about biased training data in Large Language Models like GPT. Bias in these models exists and is a real problem, one I&apos;ve &lt;a href=&quot;https://cognitiveinheritance.com/Posts/Programmers-Take-Responsibility-for-Your-AIe28099s-Output.html&quot;&gt;written about in the past&lt;/a&gt;. That post enumerates my belief that it is our responsibility as technologists to understand and work around these biases. I believe we agree in this area. I also suspect that we agree that the loud voices with something to sell are to be ignored, regardless of what they are selling. I hope we also agree that the opinions of these people should not bias our opinions in any direction. That is, just because they are saying it, doesn&apos;t make it true or false. They should be ignored, with no attention paid to them whatsoever regarding the truth of any general proposition.&lt;/p&gt;
&lt;p&gt;Where we clearly disagree is this: all of these technologies &lt;em&gt;can&lt;/em&gt; help create real value for ourselves, our users, and our society.&lt;/p&gt;
&lt;p&gt;In some cases, like with crypto currencies, that value may never be realized because the scale that is needed to be successful with it is only available to those who have already proven their desire to fleece the rest of us, and because there is no reasonable way to tell the scammers from legit-minded individuals when new products are released. There is also no mechanism to prevent a takeover of such a system by those with malicious intent. This is unfortunate, but it is the state of our very broken system.&lt;/p&gt;
&lt;p&gt;This is not the case with LLMs, and since we both understand that these models are just a very advanced version of autocomplete, we have at least part of the understanding needed to use them effectively. It seems however we disagree on what that fact (that it is an advanced autocomplete) means. It seems to me that LLMs produce derivative works in the same sense (not method) that our brains do. We, as humans, do not synthesize ideas from nothing, we build on our combined knowledge and experience, sometimes creating things heretofore unseen in that context, but always creating derivatives based on what came before.&lt;/p&gt;
&lt;p&gt;Word2Vec uses a 60-dimensional vector store. GPT-4 appears to have something north of 1500 dimensions (I&apos;ve heard differing numbers). I certainly cannot consciously think in that number of dimensions. It is plausible that my subconscious can, but that line of thinking leads to the the consideration of the nature of consciousness itself, which is not a topic I am capable of debating, and somewhat ancillary to the point, which is: these tools have value when used properly and we are the ones who can use them in valid and valuable ways.&lt;/p&gt;
&lt;p&gt;The important thing is to not listen to the loud voices. Don&apos;t even listen to me. Look at the tools and decide for yourself where you find value, if any. I suggest starting with something relatively simple, and working from there. For example, I used Bing chat during the course of this conversation to help me figure out the right words to use. I typed in a natural-language description of the word I needed, which the LLM translated into a set of possible intents. Bing then used those intents to search the internet and return results. It then used GPT to summarize those results into a short, easy to digest answer along with reference links to the source materials. I find this valuable, I think you would too. Could I have done something similar with a thesaurus, sure. Would it have taken longer: probably. Would it have resulted in the same answer: maybe. It was valuable to me to be able to describe what I needed, and then fine-tune the results, sometimes playing-off of what was returned from the earlier requests. In that way, I would call the tool a force-multiplier.&lt;/p&gt;
&lt;p&gt;Yesterday, I described a fairly complex set of things I care to read about when I read social media posts, then asked the model to evaluate a bunch of posts and tell me whether I might care about each of those posts or not. I threw a bunch of real posts at it, including many where I was trying to trick it (those that came up in typical searches but I didn&apos;t really care about, as well as the converse). It &amp;quot;understood&amp;quot; the context (probably due to the number of dimensions in the model and the relationships therein) and labeled every one correctly. I can now use an automated version of this prompt to filter the vast swaths of social media posts down to those I might care about. I could then also ask the model to give me a summary of those posts, and potentially try to synthesize new information from them. I would not make any decisions based on that summary or synthesis without first verifying the original source materials, and without reasoning on it myself, and I would not ever take any action that impacts human beings based on those results. Doing so  would be using these tools outside of their sphere of capabilities. I can however use that summary to identify places for me to drill-in and continue my evaluation, and I believe, can use them in certain circumstances to derive new ideas. This is valuable to me.&lt;/p&gt;
&lt;p&gt;So then, what should we build to leverage the capabilities of these tools to the benefit of our users, without harming other users or society? It is my opinion that, even if these tools only make it easier for us to allow our users to interact with our software in more natural ways, that is, in itself a win. These models represent a higher-level of abstraction to our programming. It is a more declarative mechanism for user interaction. With any increase in abstraction there always comes an increase in danger. As technologists it is our responsibility to understand those dangers to the best of our abilities and work accordingly. I believe we should not be dismissing tools just because they &lt;em&gt;can&lt;/em&gt; be abused, and there is no doubt that some certainly &lt;em&gt;will&lt;/em&gt; abuse them. We need to do what&apos;s right, and that may very well involve making sure these tools are used in ways that are for the benefit of the users, not their detriment.&lt;/p&gt;
&lt;p&gt;Let me say it this way: If the only choices people have are to use tools created by those with questionable intent, or to not use these tools at all, many people will choose the easy path, the one that gives them some short-term value regardless of the societal impact. If we can create value for those people without malicious intent, then the users have a choice, and will often choose those things that don&apos;t harm society. It is up to us to make sure that choice exists.&lt;/p&gt;
&lt;p&gt;I accept that you may disagree. You know that I, and all of our shared circle to the best of my knowledge, find your opinion thoughtful and valuable on many things. That doesn&apos;t mean we have to agree on everything. However, I hope that disagreement is based on far more than just the mistrust of screaming hyperbolists, and a misunderstanding of what it means to be a &amp;quot;overgrown autocomplete&amp;quot;.&lt;/p&gt;
&lt;p&gt;To be clear here, it is possible that it is I who is misunderstanding these capabilities. Obviously, I don&apos;t believe that to be the case but it is always a possibility, especially as I am not an expert in the field. Since I find the example you gave about replacing words in a Shakespearean poem to be a very obvious (to me) false analog, it is clear that at lease one of us, perhaps both of us, are misunderstanding its capabilities.&lt;/p&gt;
&lt;p&gt;I still think it would be worth your time, and a benefit to society, if people who care about the proper use of these tools, would consider how they could be used to society&apos;s benefit rather than allowing the only use to be by those who care only about extracting value from users. You have already admitted there are at least &amp;quot;one and a half valid use cases for LLMs&amp;quot;. I&apos;m guessing you would accept then that there are probably more you haven&apos;t seen yet. Knowing that, isn&apos;t it our &lt;em&gt;responsibility&lt;/em&gt; as technologists to find those uses and work toward creating the better society we seek, rather than just allowing extremists to use it to our detriment.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Update: I realize I never addressed the issue of the models being trained on licensed works.&lt;/p&gt;
&lt;p&gt;Unless a model builder has permission from a user to train their models using that user&apos;s works, be it an OSS or Copyleft license, explicit license agreement, or direct permission, those items should not be used to train models. If it is shown that a model has been trained using such data sets, and there have been indications (unproven as yet to my knowledge) that this may be the case for some models, especially image-generators, then that is a problem with those models that needs to be addressed. It does not invalidate the general use of these models, nor is it an indictment of any person or model except those in violation. Our trademark and copyright systems are another place where we, as a society, have completely fallen-down. Hopefully, that collapse will not cause us to forsake the value that these tools can provide.&lt;/p&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/continuing-a-conversation-on-llms.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/dbbcb0db-7465-4489-a07b-eae7c8716685.html</guid>
  <pubDate>Thu, 13 Apr 2023 07:00:00 GMT</pubDate>
</item><item>
  <title>Beta Tools and Wait-Lists</title>
  <description>&lt;p&gt;Here&apos;s a problem I am clearly privileged to have. I&apos;ll be working on a project and run into a problem. I search the Internet for ways to solve that problem and find a beta product that looks like a very interesting, innovative way to solve that problem. So, I sign up for the beta and end up getting put on a waitlist. This doesn&apos;t help me, at least not right now. So, I go off and find another way to solve my problem and continue doing what I&apos;m doing and forget all about the beta program that I signed up for.&lt;/p&gt;
&lt;p&gt;Then, at some point, I get an email from them saying congratulations you&apos;ve been accepted to our beta program. Well, guess what? I don&apos;t even remember who you are or what problem I was trying to solve anymore or even if I actually even signed up for it. In fact, most of the time that I get emails like that, I just assume that it is another spam email.&lt;/p&gt;
&lt;p&gt;I understand there are valid reasons for sometimes putting customers on waitlists. I also understand that sometimes companies just try to create artificial scarcity so that their product takes on a cool factor. Please know that, if this is what you&apos;re doing, you&apos;re likely losing as many customers as you would gain if not more, and may be putting your very existance at risk.&lt;/p&gt;
&lt;p&gt;I wonder how many cool products I&apos;ve missed out on because of that delay in getting access? I wonder how many cool products just died because they weren&apos;t there for people when they actually needed them.&lt;/p&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/beta-tools-and-wait-lists.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/dbbcb0db-7465-4489-a07b-eae7c8716685.html</guid>
  <pubDate>Wed, 12 Apr 2023 07:00:00 GMT</pubDate>
</item><item>
  <title>Microservices&colon; Size Doesn&apos;t Matter, Reliability Does</title>
  <description>&lt;p&gt;There are conflicting opinions among architects about how many microservices a distributed system should have, and the size of those services. Some may say that a particular design has too many microservices, and that it should be consolidated into fewer, larger services to reduce deployment and operational complexity. Others may say that the same design doesn&apos;t have enough microservices, and that it should be broken-down into smaller, more granular services to reduce code complexity and improve team agility. Aside from the always true and rarely helpful &amp;quot;it depends...&amp;quot;, is there good guidance on the subject?&lt;/p&gt;
&lt;p&gt;The truth is, the number and size of microservices is not a measure of quality or performance unto itself, it is a design decision based on one primary characteristic, &lt;strong&gt;Reliability&lt;/strong&gt;. As such, there is a simple rule guiding the creation of services, but it isn&apos;t based on the size or quantity of services. The rule is based entirely on how much work a service does.&lt;/p&gt;
&lt;p&gt;After security, reliability is the most important attribute of any system, because it affects the satisfaction of both the users and developers, as well as the productivity and agility of the development and support teams. A reliable system has the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It performs its duties as expected&lt;/li&gt;
&lt;li&gt;It has minimal failures where it has to report to the user that it is unable to perform its duties&lt;/li&gt;
&lt;li&gt;It has minimal downtime when it cannot be reached and opportunities may be lost&lt;/li&gt;
&lt;li&gt;It recovers itself automatically when outages do occur, without data loss&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Having reliable systems means that your support engineers won&apos;t be constantly woken-up in the middle of the night to deal with outages, and your customers will remain satisfied with the quality of the product.&lt;/p&gt;
&lt;h4&gt;How do we build reliable systems with microservices?&lt;/h4&gt;
&lt;p&gt;The key to building reliable systems using microservices is to follow one simple rule: &lt;strong&gt;avoid dual-writes&lt;/strong&gt;. A dual-write is when a service makes more than one change to system state within an execution context. Dual-writes are the enemy of reliability, because they create the risk of inconsistency, data loss, and data corruption.&lt;/p&gt;
&lt;p&gt;For example, a web API that updates a database and sends a message to a queue during the execution of a single web request is performing a dual-write since it is making two different changes to the state of the system, and both of the changes are expected to occur reliably. If one of the writes succeeds and the other fails, the system state becomes out of sync and system behavior becomes unpredictable. The errors created when these types of failures occur are often hard to find and remediate because they can present very differently depending on the part of the process being executed when the failure happened.&lt;/p&gt;
&lt;p&gt;The best-practice is to allow microservices to perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Idempotence&quot; title=&quot;Idempotence is the ability to execute a task an arbitrary number of times (&amp;gt;1) and have the resulting state of the system be the same as if the task was executed once.&quot;&gt;idempotent&lt;/a&gt; operations like database reads as often as they need, but to only write data once. An &lt;strong&gt;atomic update&lt;/strong&gt; to a database is an example of such a write, regardless of how many tables or collections are updated during that process. In this way, we can keep the state of each service consistent, and the system behavior deterministic. If the process fails even part-way through, we know how to recover, and can often do it automatically.&lt;/p&gt;
&lt;p&gt;Building this type of system does require a change in how we design our services. In the past, it was very common for us to make multiple changes to a system&apos;s state, especially inside a monolithic application. To remain reliable, we need to leverage tools like &lt;a href=&quot;https://en.wikipedia.org/wiki/Change_data_capture&quot; title=&quot;Events created by a database system when data updates occur that can reliably trigger downstream actions&quot;&gt;Change Data Capture (CDC)&lt;/a&gt;, which is available in most modern database systems, or the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/architecture/best-practices/transactional-outbox-cosmos&quot; title=&quot;Reliably update a data store AND take additional downstream action by atomically writing 2 updates to the data store, the state update, and an event that is monitored for by another system and used to take the additional action&quot;&gt;Transactional Outbox Pattern&lt;/a&gt; so that we can write our data once, and have that update trigger other activities downstream.&lt;/p&gt;
&lt;p&gt;Since microservices are sized to avoid dual-writes, the number of microservices in a system is determined by what they do and how they interact. The number of microservices is not a fixed or arbitrary number, but a result of the system design and the business needs. By following the rule of avoiding dual-writes, you can size your microservices appropriately, and achieve a system that is scalable and adaptable, but most of all, reliable. Of course, this practice alone will not guarantee the reliability of your systems, but it will make reliability possible, and is the best guideline I&apos;ve found for sizing microservices.&lt;/p&gt;
&lt;p&gt;For more detail on how to avoid the Dual-Writes Anti-Pattern, please see my article from December 2022 on &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-1of6-context.html&quot;&gt;The Execution Context&lt;/a&gt;.&lt;/p&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/microservices-size-does-not-matter.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/53e0a79d-1096-4874-9639-6c1fb3eb8b47.html</guid>
  <pubDate>Mon, 20 Feb 2023 07:00:00 GMT</pubDate>
</item><item>
  <title>Simple Linear Regression</title>
  <description>&lt;p&gt;My high-school chemistry teacher, Mrs. J, had a name for that moment when she could see the lightbulb go on over your head. You know, that instant where realization hits and a concept sinks-in and becomes part of your consciousness. The moment that you truly &amp;quot;Grok&amp;quot; a principle. She called that an &amp;quot;aha experience&amp;quot;.&lt;/p&gt;
&lt;p&gt;One of my favorite &amp;quot;aha experiences&amp;quot; from my many years as a Software Engineer, is when I realized that the simplest neural network, a model with one input and one output, was simply modeling a line, and that training such a model, was just performing a linear regression. Mind. Blown.&lt;/p&gt;
&lt;p&gt;In case you haven&apos;t had this particular epiphany yet, allow me to go into some detail. I also discuss this in my conference talk, &lt;a href=&quot;https://cognitiveinheritance.com/Pages/Speaking-Engagements.html#ai-talks&quot;&gt;A Developer&apos;s Introduction to Artificial Intelligences&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Use Case: Predict the Location of a Train&lt;/h4&gt;
&lt;p&gt;Let&apos;s use the example of predicting the location of a train. Because they are on rails, trains move in 1-dimensional space. We can get a good approximation of their movement, especially between stops, by assuming they travel at a consistent speed. As a result, we can make a reasonably accurate prediction of a train&apos;s distance from a particular point on the rail, using a linear equation.&lt;/p&gt;
&lt;p&gt;If we have sensors reporting the location and time of detection of our train, spread-out across our fictional rail system, we might be able to build a graph of these reports that looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.cognitiveinheritance.com/Images/Positive Only Linear Data.png&quot; alt=&quot;A plot of data points that, while noisy, clearly start from the origin at the bottom-left and moves in a line toward the top-right&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I think it is clear that this data can be represented using a &amp;quot;best-fit line&amp;quot;. Certainly there is some error in the model, perhaps due to sensor or reporting errors, or maybe just to normal variance of the data. However, there can be no doubt that the best fit for this data would be represented as a line. In fact, there are a number of tools that can make it very easy to generate such a line. But what does that line really represent? To be a &amp;quot;best-fit&amp;quot;, the line needs to be drawn in such a way as to minimize the differences between the values found in the data and the values on the line. Thus, the total error between the values predicted by our best-fit line, and the actual values that we measured, is as small as we can possibly get it.&lt;/p&gt;
&lt;h4&gt;A Linear Neural Network&lt;/h4&gt;
&lt;p&gt;A simple neural network, one without any hidden layers, consists of one or more input nodes, connected with edges to one or more output nodes. Each of the edges has a weight and each output node has a bias. The values of the output nodes are calculated by summing the product of each input connected to it, along with its corresponding weight, and adding in the output node&apos;s bias. Let&apos;s see what our railroad model might look like using a simple neural network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.cognitiveinheritance.com/Images/Simplest Neural Network.png&quot; alt=&quot;The simplest neural network; 1 input node, connected to 1 output node via 1 edge&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Ours is the simplest possible neural network, one input connected to one output, where our &lt;strong&gt;X&lt;/strong&gt; value (time) is the input and the output &lt;strong&gt;Y&lt;/strong&gt; is our prediction of the distance the train has traveled in that time. To make the best prediction we need to determine the values for the weight of the edge &lt;strong&gt;m&lt;/strong&gt; and the bias of the output node &lt;strong&gt;b&lt;/strong&gt; that produce the output that minimizes the errors in the model.&lt;/p&gt;
&lt;p&gt;The process of finding the weights and biases values for a neural network that minimize the error is know as &lt;em&gt;Training&lt;/em&gt; the model. Once these values are determined, we use the fact that we multiply the weight by the input (m * X) and add in the bias. This gives us an equation in the form:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Y = mX + b&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You may recognize this as the &lt;strong&gt;slope-intercept&lt;/strong&gt; form of the equation for a line, where the slope &lt;strong&gt;m&lt;/strong&gt; represents the speed of the train, and the bias &lt;strong&gt;b&lt;/strong&gt; represents the starting distance from the origin. Once our training process gives us values for &lt;strong&gt;m&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt;, we can easily plug-in any value for &lt;strong&gt;X&lt;/strong&gt; and get a prediction for the location of our train.&lt;/p&gt;
&lt;h4&gt;Training a Model&lt;/h4&gt;
&lt;p&gt;Training an AI model is simply finding the set of parameters that minimize the difference between the predicted output and the actual output. This is key to understanding AI - it&apos;s all about minimizing the error. Error minimization is the exact same goal as we have when performing a linear regression, which makes sense since these regressions are predictive models on their own, they just aren&apos;t generally depicted as neural networks.&lt;/p&gt;
&lt;p&gt;There are many ways to perform the error-minimization process. Many more complicated models are trained using an iterative optimization routine called &lt;strong&gt;Gradient Descent&lt;/strong&gt;. Extremely simple models like this one often use a less complicated process such as &lt;strong&gt;Ordinary Least Squares&lt;/strong&gt;. The goals are the same however, values for weights and biases in the model are found that minimize the error in the output, resulting in a model can make the desired predictions based on known inputs.&lt;/p&gt;
&lt;p&gt;Regardless of the method used, the realization that training the simplest neural network results in a model of a line provided the &amp;quot;aha experience&amp;quot; I needed as the foundation for my understanding of Machine Learning models. I hope, by publishing this article, that others may also benefit from this recognition.&lt;/p&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/simple-linear-regression.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/18a693d3-448c-4477-b4e2-7193e896a2b6.html</guid>
  <pubDate>Mon, 13 Feb 2023 07:00:00 GMT</pubDate>
</item><item>
  <title>Like a River</title>
  <description>&lt;p&gt;We all understand to some degree, that the metaphor comparing the design and construction of software to that of a building is flawed at best. That isn&apos;t to say it&apos;s useless of course, but it seems to fail in at least one critical way; it doesn&apos;t take into account that creating software should be solving a business problem that has never been solved before. Sure, there are patterns and tools that can help us with technical problems similar to those that have been solved in the past, but we should not be solving the same business problem over and over again. If we are, we are doing something very wrong. Since our software cannot simply follow long-established plans and procedures, and can evolve very rapidly, even during construction, the over-simplification of our processes by excluding the innovation and problem-solving aspects of our craft, feels rather dangerous.&lt;/p&gt;
&lt;h4&gt;Like Constructing a Building&lt;/h4&gt;
&lt;p&gt;It seems to me that by making the comparison to building construction, we are over-emphasizing the scientific aspects of software engineering, and under-emphasizing the artistic ones. That is, we don&apos;t put nearly enough value on innovation such as designing abstractions for testability and extensibility. We also don&apos;t emphasize enough the need to understand the distinct challenges of our particular problem domain, and how the solution to a similar problem in a different domain may focus on the wrong features of the problem. As an example, let&apos;s take a workforce scheduling tool. The process of scheduling baristas at a neighborhood coffee shop is fundamentally similar to one scheduling pilots to fly for a small commercial airline. However, I probably don&apos;t have to work too hard to convince you that the two problems have very different characteristics when it comes to determining the best solutions. In this case, the distinctions are fairly obvious, but in many cases they are not.&lt;/p&gt;
&lt;p&gt;Where the architecture metaphor makes the most sense to me is in the user-facing aspects of both constructions. The physical aesthetics, as well as the experience humans have in their interactions with the features of the design are critical in both scenarios, and in both cases will cause real problems if ignored or added as an afterthought. Perhaps this is why the architecture metaphor has become so prevalent in that it is easy to see the similarities between the aesthetics and user-experience of buildings and software, even for a non-technical audience. However, most software built today has a much cleaner separation of concerns than software built when this metaphor was becoming popular in the 1960s and 70s, rendering it mostly obsolete for the vast majority of our systems and sub-systems.&lt;/p&gt;
&lt;p&gt;When we consider more technical challenges such as design for reliability and resiliency, the construction metaphor fails almost completely. Reliability is far more important in the creation of buildings than it is in most software projects, and often very different. While it is never ok for the structure of a building to fail, it can be perfectly fine, and even expected, for most aspects of a software system to fail occasionally, as long as those failures are well-handled. Designing these mechanisms is a much more flexible and creative process in building software, and requires a large degree of innovation to solve these problems in ways that work for each different problem domain. Even though the two problems can share the same name in software and building construction, and have some similar characteristics, they are ultimately very different problems and should be seen as such. The key metaphors we use to describe our tasks should reflect these differences.&lt;/p&gt;
&lt;h4&gt;Like a River&lt;/h4&gt;
&lt;p&gt;For more than a decade now, I&apos;ve been fascinated by Grady Booch&apos;s suggestion that a more apt metaphor for the structure and evolution of the software within an enterprise is that of a river and its surrounding ecosystem &lt;a href=&quot;https://www.computer.org/csdl/magazine/so/2009/03&quot;&gt;G. Booch, &amp;quot;Like a River&amp;quot; in IEEE Software, vol. 26, no. 03, pp. 10-11, 2009&lt;/a&gt;. In this abstraction, bank-to-bank slices represent the current state of our systems, while upstream-downstream sections represent changes over time. The width and depth of the river represent the breadth and depth of the structures involved, while the speed of the water, and the differences in speed between the surface (UI) and depths (back-end) represent the speed of changes within those sub-systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The life cycle of a software-intensive system is like a river, and we, as developers, are but captains of the boats that ply its waters and dredge its channels. - Grady Booch&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I will not go into more detail on Booch&apos;s analogy, since it will be far better to read it for yourself, or &lt;a href=&quot;https://www.computer.org/publications/tech-news/on-architecture/like-a-river&quot;&gt;hear it in his own voice&lt;/a&gt;. I will however point out that, in his model, Software Engineers are &amp;quot;…captains of the boats that ply the waters and dredge the channels&amp;quot;. It is in this context, that I find the river metaphor most satisfying.&lt;/p&gt;
&lt;p&gt;As engineers, we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Navigate and direct the flow of software development, just as captains steer their boats ina particular direction.&lt;/li&gt;
&lt;li&gt;Make decisions and take action to keep the development process moving forward, similar to how captains navigate their boats through obstacles and challenges.&lt;/li&gt;
&lt;li&gt;Maintain a highly-functional anomaly detection and early-warning system to alert us of upcoming obstacles such as attacks and system outages, similar to the way captains use sonar to detect underwater obstacles and inspections by their crew, to give them useful warnings.&lt;/li&gt;
&lt;li&gt;Use ingenuity and skill, while falling back on first-principles, to know when to add abstractions or try something new, in the same way that captains follow the rules of seamanship, but know when to take evasive or unusual action to protect their charge.&lt;/li&gt;
&lt;li&gt;Maintain a good understanding of the individual components of the software, as well as the broader architecture and how each component fits within the system, just as captains need to know both the river and its channels, and the details of the boat on which they travel.&lt;/li&gt;
&lt;li&gt;Are responsible for ensuring the software is delivered on time and within budget, similar to how captains ensure their boats reach their destination on schedule.&lt;/li&gt;
&lt;li&gt;May be acting on but one small section at a time of the broader ecosystem. That is, an engineer may be working on a single feature, and make decisions on how that element is implemented, while other engineers act similarly on other features. This is akin to the way many captains may navigate the same waters simultaneously on different ships, and must make decisions that take into account the presence, activities and needs of the others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This metaphor, in my opinion, does a much better job of identifying the critical nature of the software developer in the design of our software than then that of the creation of a building structure. It states that our developers are not merely building walls, but they are piloting ships, often through difficult waters that have never previously been charted. These are not laborers, but knowledge-workers whose skills and expertise need to be valued and depended on.&lt;/p&gt;
&lt;p&gt;Unfortunately this metaphor, like all others, is imperfect. There are a number of elements of software engineering where no reasonable analog exists into the world of a riverboat captain. One example is the practice of pair or mob programming. I don&apos;t recall ever hearing of any instances where a pair or group of ships captains worked collaboratively, and on equal footing, to operate a single ship. Likewise, the converse is also true. I know of no circumstances in software engineering where split-second decisions can have life-or-death consequences. That said, I think the captain metaphor does a far better job of describing the skill and ingenuity required to be a software engineer than that of building construction.&lt;/p&gt;
&lt;p&gt;To be very clear, I am not saying that the role of a construction architect, or even construction worker, doesn&apos;t require skill and ingenuity, quite the contrary. I am suggesting that the types of skills and the manner of ingenuity required to construct a building, doesn&apos;t translate well in metaphor to that required of a software engineer, especially to those who are likely to be unskilled in both areas. It is often these very people, our management and leadership, whom these metaphors are intended to inform. Thus, the construction metaphor represents the job of a software developer ineffectively.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;The comparisons of creating software to creating an edifice is not going away any time soon. Regardless of its efficacy, this model has come to be part of our corporate lexicon and will likely remain so for the foreseeable future. Even the title of &amp;quot;Software Architect&amp;quot; is extremely prevalent in our culture, a title which I have held, and a role that I have enjoyed for many years now. That said, it could only benefit our craft to make more clear the ways in which that metaphor fails. This clarity would benefit not just the non-technical among us who have little basis to judge our actions aside from these metaphors, but also us as engineers. It is far too easy for anyone to start to view developers as mere bricklayers, rather than the ships captains we are. This is especially true when generations of engineers have been brought up on and trained on the architecture metaphor. If they think of themselves as just workers of limited, albeit currently valuable skill, it will make it much harder for them to challenge those things in our culture that need to be challenged, and to prevent the use of our technologies for nefarious purposes.&lt;/p&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/like-a-river.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/472f33ba-2519-4fa0-84c1-f1dc869d4cba.html</guid>
  <pubDate>Mon, 06 Feb 2023 07:00:00 GMT</pubDate>
</item><item>
  <title>Microservices - Not Just About Scalability</title>
  <description>&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt; is an important feature of microservices and event-driven architectures, however it is only one of the many benefits these types of architectures provide. Event-driven designs create systems with high availability and fault tolerance, as well as improvements for the development teams such as flexibility in technology choices and the ability to subdivide tasks better. These features can help make systems more robust and reliable, and have a great impact on development team satisfaction. It is important to consider these types of architectures not just for systems that need to scale to a high degree, but for any system where reliability or complexity are a concern.&lt;/p&gt;
&lt;p&gt;The reliability of microservices come from the fact that they break-down monolithic applications into smaller, independently deployable services. When implemented properly this approach allows for the isolation of failures, where the impact of a failure in one service can be limited to that service and its consumers, rather than cascading throughout the entire system. Additionally, microservice architectures enable much easier rollbacks, where if a new service version has a bug, it can be rolled back to a previous version without affecting other services. Event-driven approaches also decouple services by communicating through events rather than direct calls, making it easier to change or replace them without affecting other services. Perhaps most importantly, microservice architectures help reliability by avoiding dual-writes. Ensuring that our services make at most one state change per execution context allows us to avoid the very painful inconsistencies that can occur when data is written to multiple locations simultaneously and these updates are only partially successful.&lt;/p&gt;
&lt;p&gt;When asynchronous eventing is used rather than request-response messages, these systems are further decoupled in time, improving fault-tolerance and allowing the systems to self-heal from failures in downstream dependencies. Microservices also enable fault-tolerance in our services by making it possible for some of our services to be idempotent or even fully stateless. Idempotent services can be called repeatedly without additional side-effects, making it easy to recover from failures that occur during our processes.&lt;/p&gt;
&lt;p&gt;Finally, microservices improve the development and support process by enabling modularity and allowing each team to use the tools and technologies they prefer. Teams can work on smaller, independent parts of the system, reducing coordination overhead and enabling faster time-to-market for new features and improvements. Each service can be deployed and managed separately, making it easier to manage resource usage and address problems as they arise. These architectures provide greater flexibility and agility, allowing teams to focus on delivering value to the business without being bogged down by the constraints of a monolithic architecture.&lt;/p&gt;
&lt;p&gt;While it is true that most systems won&apos;t ever need to scale to the point that they &lt;em&gt;require&lt;/em&gt; a microservices architecture, many of these same systems do need the reliability and self-healing capabilities modern architectures provide. Additionally, everyone wants to work on a development team that is efficient, accomplishes their goals, and doesn&apos;t constantly force them to wake up in the middle of the night to handle support issues.&lt;/p&gt;
&lt;p&gt;If you have avoided using event-driven microservices because scalability isn&apos;t one of the key features of your application, I encourage you to explore the many other benefits of these architectures.&lt;/p&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/microservices-not-just-about-scale.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/fb1cfc1d-1635-45f9-954f-772527d43661.html</guid>
  <pubDate>Mon, 30 Jan 2023 07:00:00 GMT</pubDate>
</item><item>
  <title>Critical Questions to Ask Your Team About Microservices</title>
  <description>&lt;p&gt;Over the last 6 weeks we have discussed the creation, maintenance and operations of microservices and event-driven systems. We explored different conversations that development teams should have prior to working with these types of architectures. Asking the questions we outlined, and answering as many of them as are appropriate, will help teams determine which architectural patterns are best for them, and assist in building their systems and processes in a reliable and supportable way. These conversations are known as &amp;quot;&lt;strong&gt;The Critical C&apos;s of Microservices&lt;/strong&gt;&amp;quot;, and each is detailed individually in its own article.&lt;/p&gt;
&lt;p&gt;The &amp;quot;Critical C&apos;s&amp;quot; are: &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-1of6-context.html&quot;&gt;Context&lt;/a&gt;, &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-2of6-consistency.html&quot;&gt;Consistency&lt;/a&gt;, &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-3of6-contract.html&quot;&gt;Contract&lt;/a&gt;, &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-4of6-chaos.html&quot;&gt;Chaos&lt;/a&gt;, &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-5of6-competencies.html&quot;&gt;Competencies&lt;/a&gt; and &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-6of6-coalescence.html&quot;&gt;Coalescence&lt;/a&gt;. For easy reference, I have aggregated all of the key elements of each conversation in this article. For details about &lt;strong&gt;why&lt;/strong&gt; each is important, please consult the article specific to that topic.&lt;/p&gt;
&lt;p&gt;There is also a  &lt;a href=&quot;https://www.cognitiveinheritance.com//CriticalCs/index.html&quot;&gt;Critical C&apos;s of Microservices&lt;/a&gt; website that includes the same information as in these articles. This site will be kept up-to-date as the guidance evolves.&lt;/p&gt;
&lt;h4&gt;Questions about Context&lt;/h4&gt;
&lt;p&gt;Development teams should have conversations around &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-1of6-context.html&quot;&gt;Context&lt;/a&gt; that are primarily focused around the tools and techniques that they intend to use to avoid the Dual-Writes Anti-Pattern. These conversations should include answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What database technologies will we use and how can we leverage these tools to create downstream events based on changes to the database state?&lt;/li&gt;
&lt;li&gt;Which of our services are currently idempotent and which ones could reasonably made so? How can we leverage our idempotent services to improve system reliability?&lt;/li&gt;
&lt;li&gt;Do we have any services right now that contain business processes implemented in a less-reliable way? If so, pulling this functionality out into their own microservices might be a good starting point for decomposition.&lt;/li&gt;
&lt;li&gt;What processes will we as a development team implement to track and manage the technical debt of having business processes implemented in a less-reliable way?&lt;/li&gt;
&lt;li&gt;What processes will we implement to be sure that any future less-reliable implementations of business functionality are made with consideration and understanding of the debt being created and a plan to pay it off.&lt;/li&gt;
&lt;li&gt;What processes will we implement to be sure that any existing or future less-reliable implementations of business functionality are documented, understood by, and prioritized by the business process owners.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Questions about Consistency&lt;/h4&gt;
&lt;p&gt;Development teams should have conversations around &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-2of6-consistency.html&quot;&gt;Consistency&lt;/a&gt; that are primarily focused around making certain that the system is assumed to be eventually consistency throughout. These conversations should include answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What patterns and tools will we use to create systems that support reliable, eventually consistent operations?&lt;/li&gt;
&lt;li&gt;How will we identify existing areas where higher-levels of consistency have been wedged-in and should be removed?&lt;/li&gt;
&lt;li&gt;How will we prevent future demands for higher-levels of consistency, either explicit or assumed, to creep in to our systems?&lt;/li&gt;
&lt;li&gt;How will we identify when there are unusual or unacceptable delays in the system reaching a consistent state?&lt;/li&gt;
&lt;li&gt;How will we communicate the status of the system and any delays in reaching a consistent state to the relevant stakeholders?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Questions about Contract&lt;/h4&gt;
&lt;p&gt;Development teams should have conversations around &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-3of6-contract.html&quot;&gt;Contract&lt;/a&gt; that are primarily focused around creating processes that define any integration contracts for both upstream and downstream services, and serve to defend their internal data representations against any external consumers. These conversations should include answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How will we isolate our internal data representations from those of our downstream consumers?&lt;/li&gt;
&lt;li&gt;What types of compatibility guarantees are our tools and practices capable of providing?&lt;/li&gt;
&lt;li&gt;What procedures should we have in place to monitor incoming and outgoing contracts for compatibility?&lt;/li&gt;
&lt;li&gt;What should our procedures look like for making a change to a stream that has downstream consumers?&lt;/li&gt;
&lt;li&gt;How can we leverage upstream messaging contracts to further reduce the coupling of our systems to our upstream dependencies?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Questions about Chaos&lt;/h4&gt;
&lt;p&gt;Development teams should have conversations around &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-4of6-chaos.html&quot;&gt;Chaos&lt;/a&gt; that are primarily focused around procedures for identifying and remediating possible failure points in the application. These conversations should include answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How will we evaluate potential sources of failures in our systems before they are built?
&lt;ul&gt;
&lt;li&gt;How will we handle the inability to reach a dependency such as a database?&lt;/li&gt;
&lt;li&gt;How will we handle duplicate messages sent from our upstream data sources?&lt;/li&gt;
&lt;li&gt;How will we handle messages sent out-of-order from our upstream data sources?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How will we expose possible sources of failures during any pre-deployment testing?&lt;/li&gt;
&lt;li&gt;How will we expose possible sources of failures in the production environment before they occur for users?&lt;/li&gt;
&lt;li&gt;How will we identify errors that occur for users within production?&lt;/li&gt;
&lt;li&gt;How will we prioritize changes to the system based on the results of these experiments?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Questions about Competencies&lt;/h4&gt;
&lt;p&gt;Development teams should have conversations around &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-5of6-competencies.html&quot;&gt;Competencies&lt;/a&gt; that are primarily focused around what systems, sub-systems, and components should be built, which should be installed off-the-shelf, and what libraries or infrastructure capabilities should be utilized. These conversations should include answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are our core competencies?&lt;/li&gt;
&lt;li&gt;How do we identify &amp;quot;build vs. buy&amp;quot; opportunities?&lt;/li&gt;
&lt;li&gt;How do we make &amp;quot;build vs. buy&amp;quot; decisions on needed systems?&lt;/li&gt;
&lt;li&gt;How do we identify cross-cutting concerns and infrastructure capabilites that can be leveraged?&lt;/li&gt;
&lt;li&gt;How do we determine which libraries or infrastructure components will be utilized?&lt;/li&gt;
&lt;li&gt;How do we manage the versioning of utilized components, especially in regard to security updates?&lt;/li&gt;
&lt;li&gt;How do we document our decisions for later review?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Questions about Coalescence&lt;/h4&gt;
&lt;p&gt;Development teams should have conversations around &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-6of6-coalescence.html&quot;&gt;Coalescence&lt;/a&gt; that are primarily focused around brining critical information about the operation of our systems together for easy access. These conversations should include answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is our mechanism for deployment and system verification?
&lt;ul&gt;
&lt;li&gt;How will we identify, as quickly as possible, when a deployment has had a negative impact on our system?&lt;/li&gt;
&lt;li&gt;Are there tests that can validate the operation of the system end-to-end?&lt;/li&gt;
&lt;li&gt;How will we surface the status of any deployment and system verification tests?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What is our mechanism for logging/traceability within our system?
&lt;ul&gt;
&lt;li&gt;How will we coalesce our logs from the various services within the system?&lt;/li&gt;
&lt;li&gt;How will we know if there are anomalies in our logs?&lt;/li&gt;
&lt;li&gt;Are there additional identifiers we need to add to allow traceability?&lt;/li&gt;
&lt;li&gt;Are there log queries that, if enabled, might provide additional support during an outage?&lt;/li&gt;
&lt;li&gt;Are there ways to increase the level of logging when needed to provide additional information and can this be done wholistically on the system?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How will we expose SLIs and other metrics so they are available when needed?&lt;/li&gt;
&lt;li&gt;How will we know when there are anomalies in our metrics?&lt;/li&gt;
&lt;li&gt;What are the metrics that would be needed in an outage and how will we surface those for easy access?&lt;/li&gt;
&lt;li&gt;Are there additional metrics that, if enabled, might provide additional support during an outage?&lt;/li&gt;
&lt;li&gt;Are there ways to perform ad-hoc queries against SLIs and metrics to provide additional insight in an outage?&lt;/li&gt;
&lt;li&gt;How will we identify the status of dependencies so we can understand when our systems are reacting to downstream anomalies?
&lt;ul&gt;
&lt;li&gt;How will we surface dependency status for easy access during an outage?&lt;/li&gt;
&lt;li&gt;Are there metrics we can surface for our dependencies that might help during an outage?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/critical-questions-about-microservice.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/082c3f8a-ac4a-4231-9962-01536167e4a5.html</guid>
  <pubDate>Mon, 23 Jan 2023 07:00:00 GMT</pubDate>
</item><item>
  <title>The Critical C's of Microservices - Coalescence</title>
  <description>&lt;p&gt;&amp;quot;&lt;strong&gt;The Critical C&apos;s of Microservices&lt;/strong&gt;&amp;quot; are a series of conversations that development teams should have around building event-driven or other microservice based architectures. These topics will help teams determine which architectural patterns are best for them, and assist in building their systems and processes in a reliable and supportable way.&lt;/p&gt;
&lt;p&gt;The &amp;quot;Critical C&apos;s&amp;quot; are: &lt;strong&gt;Context&lt;/strong&gt;, &lt;strong&gt;Consistency&lt;/strong&gt;, &lt;strong&gt;Contract&lt;/strong&gt;, &lt;strong&gt;Chaos&lt;/strong&gt;, &lt;strong&gt;Competencies&lt;/strong&gt; and &lt;strong&gt;Coalescence&lt;/strong&gt;. Each of these topics has been covered in detail in this series of 6 articles. The first article of the 6 was on the subject of &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-1of6-context.html&quot;&gt;Context&lt;/a&gt;. This is the final article in the series, and covers the topic of &lt;strong&gt;Coalescence&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;Coalescence&lt;/h4&gt;
&lt;p&gt;The use of Microservices reduces the complexity of our services in many ways, however it also adds complexity when it comes to deployment and operations. More services mean more deployments, even as each of those deployments is smaller and more isolated. Additionally, they can be harder on operations and support teams since there can be many more places to go when you need to find information. Ideally, we would coalesce all of the necessary information to operate and troubleshoot our systems in a single pane-of-glass so that our operations and support engineers don&apos;t have to search for information in a crisis.&lt;/p&gt;
&lt;p&gt;Deployment and system verification testing can help us identify when there are problems at any point in our system and give us insight into what the problems might be and what caused them. Tests run immediately after any deployment can help identify when a particular deployment has caused a problem so it can be addressed quickly. Likewise, ongoing system verification tests can give early indications of problems irrespective of the cause. Getting information about the results of these tests quickly and easily into the hands of the engineers that can act on them can reduce costs and prevent outages.&lt;/p&gt;
&lt;p&gt;Logging and traceability is generally considered a solved problem, so long as it is used effectively. We need to setup our systems to make the best use of our distributed logging systems. This often means adding a correlation identifier alongside various request and causation ids to make it easy to trace requests through the system. We also need to be able to monitor and surface our logs so that unusual activity can be recognized and acted on as quickly as possible.&lt;/p&gt;
&lt;p&gt;Service Level Indicators (SLIs) and other metrics can provide key insights into the operations of our systems, even if no unusual activity is seen within our logs. Knowing what operational metrics suggest there might be problems within our systems, and monitoring changes to those metrics for both our services and our dependencies can help identify, troubleshoot and even prevent outages. Surfacing those metrics for easy access can give our support and operations engineers the tools they need to do their jobs effectively.&lt;/p&gt;
&lt;h4&gt;Goals of the Conversation&lt;/h4&gt;
&lt;p&gt;Development teams should have conversations around &lt;strong&gt;Coalescence&lt;/strong&gt; that are primarily focused around brining critical information about the operation of our systems together for easy access. These conversations should include answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is our mechanism for deployment and system verification?
&lt;ul&gt;
&lt;li&gt;How will we identify, as quickly as possible, when a deployment has had a negative impact on our system?&lt;/li&gt;
&lt;li&gt;Are there tests that can validate the operation of the system end-to-end?&lt;/li&gt;
&lt;li&gt;How will we surface the status of any deployment and system verification tests?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What is our mechanism for logging/traceability within our system?
&lt;ul&gt;
&lt;li&gt;How will we coalesce our logs from the various services within the system?&lt;/li&gt;
&lt;li&gt;How will we know if there are anomalies in our logs?&lt;/li&gt;
&lt;li&gt;Are there additional identifiers we need to add to allow traceability?&lt;/li&gt;
&lt;li&gt;Are there log queries that, if enabled, might provide additional support during an outage?&lt;/li&gt;
&lt;li&gt;Are there ways to increase the level of logging when needed to provide additional information and can this be done wholistically on the system?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How will we expose SLIs and other metrics so they are available when needed?&lt;/li&gt;
&lt;li&gt;How will we know when there are anomalies in our metrics?&lt;/li&gt;
&lt;li&gt;What are the metrics that would be needed in an outage and how will we surface those for easy access?&lt;/li&gt;
&lt;li&gt;Are there additional metrics that, if enabled, might provide additional support during an outage?&lt;/li&gt;
&lt;li&gt;Are there ways to perform ad-hoc queries against SLIs and metrics to provide additional insight in an outage?&lt;/li&gt;
&lt;li&gt;How will we identify the status of dependencies so we can understand when our systems are reacting to downstream anomalies?
&lt;ul&gt;
&lt;li&gt;How will we surface dependency status for easy access during an outage?&lt;/li&gt;
&lt;li&gt;Are there metrics we can surface for our dependencies that might help during an outage?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-6of6-coalescence.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/a0280db3-6d32-4c89-81c4-10ce2cd3522d.html</guid>
  <pubDate>Mon, 16 Jan 2023 07:00:00 GMT</pubDate>
</item><item>
  <title>The Critical C's of Microservices - Competencies</title>
  <description>&lt;p&gt;&amp;quot;&lt;strong&gt;The Critical C&apos;s of Microservices&lt;/strong&gt;&amp;quot; are a series of conversations that development teams should have around building event-driven or other microservice based architectures. These topics will help teams determine which architectural patterns are best for them, and assist in building their systems and processes in a reliable and supportable way.&lt;/p&gt;
&lt;p&gt;The &amp;quot;Critical C&apos;s&amp;quot; are: &lt;strong&gt;Context&lt;/strong&gt;, &lt;strong&gt;Consistency&lt;/strong&gt;, &lt;strong&gt;Contract&lt;/strong&gt;, &lt;strong&gt;Chaos&lt;/strong&gt;, &lt;strong&gt;Competencies&lt;/strong&gt; and &lt;strong&gt;Coalescence&lt;/strong&gt;. Each of these topics will be covered in detail in this series of articles. The first article of the 6 was on the subject of &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-1of6-context.html&quot;&gt;Context&lt;/a&gt;. This is article 5 of the series, and covers the topic of &lt;strong&gt;Competencies&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;Competencies&lt;/h4&gt;
&lt;p&gt;It is our responsibility as engineers to spend our limited resources on those things that give the companies we are building for a competitive advantage in the market. This means limiting our software builds to areas where we can differentiate that company from others. Not every situation requires us to build a custom solution, and even when we do, there is usually no need for us to build every component of that system.&lt;/p&gt;
&lt;p&gt;If the problem we are solving is a common one that many companies deal with, and our solution does not give us a competitive advantage over those other companies, we are probably better off using an off-the-shelf product, whether that is a commercial (COTS) product, or a Free or Open-Source one (FOSS). Software we build should be unique to the company it is being built for, and provide that company with a competitive advantage. There is no need for us to build another Customer Relationship Manager (CRM) or Accounting system since these systems implement solutions to solved problemns that are generally solved in the same way by everyone. We should only build custom solutions if we are doing something that has never been done before or we need to do things in a way that is different from everyone else and can&apos;t be done using off-the-shelf systems.&lt;/p&gt;
&lt;p&gt;We should also only be building custom software when the problem being solved is part of our company&apos;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Core_competency&quot; title=&quot;A combination of multiple resources and skills that distinguish a firm in the marketplace and therefore are the foundation of companies&apos; competitiveness&quot;&gt;core competencies&lt;/a&gt;. If we are doing this work for a company that builds widgets, it is unlikely, though not impossible, that building a custom solution for getting parts needed to build the widgets will provide that company with a competitive advantage. We are probably better off if we focus our efforts on software to help make the widgets in ways that are better, faster or cheaper.&lt;/p&gt;
&lt;p&gt;If our &amp;quot;build vs. buy&amp;quot; decision is to build a custom solution, there are likely to be opportunities within those systems to use pre-existing capabilities rather than writing everything from scratch. For example, many cross-cutting concerns within our applications have libraries that support them very effectively. We should not be coding our own implementations for things like &lt;a href=&quot;https://en.wikipedia.org/wiki/Logging_(computing)&quot; title=&quot;The act of keeping a log of events that occur in a computer system, such as problems, errors or just information on current operation&quot;&gt;logging&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_configuration&quot; title=&quot;The arrangement of a system&apos;s functional units according to their nature, number and chief characteristics&quot;&gt;configuration&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Security&quot; title=&quot;Protection from or resilience against, potential harm (or other unwanted coercive change) caused by others, by restraining the freedom of others to act&quot;&gt;security&lt;/a&gt;. Likewise, there are many capabilities that already exist in our infrastructure that we should take advantage of. &lt;a href=&quot;https://en.wikipedia.org/wiki/Encryption&quot; title=&quot;The process of encoding information&quot;&gt;Encryption&lt;/a&gt;, which is often a capability of the operating system, is one that springs to mind. We should certainly never &amp;quot;roll-our-own&amp;quot; for more complex infrastructure features like &lt;a href=&quot;https://en.wikipedia.org/wiki/Replication_(computing)&quot; title=&quot;Sharing information so as to ensure consistency between redundant resources, such as software or hardware components, to improve reliability, fault-tolerance, or accessibility.&quot;&gt;Replication&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Change_data_capture&quot; title=&quot;CDC is a set of software design patterns used to determine and track the data that has changed so that action can be taken using the changed data.&quot;&gt;Change Data Capture&lt;/a&gt;, but might even want to consider avoiding rebuilding infrastructure capabilities that we more commonly build. An example of this might be if we would typically build a &lt;a href=&quot;https://en.wikipedia.org/wiki/Web_API&quot; title=&quot;An Application Programming Interface exposed over HTTP(S)&quot;&gt;Web API&lt;/a&gt; for our systems, we might consider exposing the API&apos;s of our backing infrastructure components instead, properly isolated and secured of course, perhaps via an &lt;a href=&quot;https://en.wikipedia.org/wiki/API_management&quot; title=&quot;The process of creating and publishing web application programming interfaces (APIs), enforcing their usage policies, controlling access, nurturing the subscriber community, collecting and analyzing usage statistics, and reporting on performance&quot;&gt;API Management&lt;/a&gt; component.&lt;/p&gt;
&lt;h4&gt;Goals of the Conversation&lt;/h4&gt;
&lt;p&gt;Development teams should have conversations around &lt;strong&gt;Competencies&lt;/strong&gt; that are primarily focused around what systems, sub-systems, and components should be built, which should be installed off-the-shelf, and what libraries or infrastructure capabilities should be utilized. These conversations should include answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are our core competencies?&lt;/li&gt;
&lt;li&gt;How do we identify &amp;quot;build vs. buy&amp;quot; opportunities?&lt;/li&gt;
&lt;li&gt;How do we make &amp;quot;build vs. buy&amp;quot; decisions on needed systems?&lt;/li&gt;
&lt;li&gt;How do we identify cross-cutting concerns and infrastructure capabilites that can be leveraged?&lt;/li&gt;
&lt;li&gt;How do we determine which libraries or infrastructure components will be utilized?&lt;/li&gt;
&lt;li&gt;How do we manage the versioning of utilized components, especially in regard to security updates?&lt;/li&gt;
&lt;li&gt;How do we document our decisions for later review?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Next Up - Coalescence&lt;/h4&gt;
&lt;p&gt;In the final article of this series we will look at &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-6of6-coalescence.html&quot;&gt;Coalescence&lt;/a&gt; and how we should work to bring all of the data together for our operations &amp;amp; support engineers.&lt;/p&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-5of6-competencies.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/a99861ed-9829-4110-9e2f-1e75efcb43a2.html</guid>
  <pubDate>Mon, 09 Jan 2023 07:00:00 GMT</pubDate>
</item><item>
  <title>The Critical C's of Microservices - Chaos</title>
  <description>&lt;p&gt;&amp;quot;&lt;strong&gt;The Critical C&apos;s of Microservices&lt;/strong&gt;&amp;quot; are a series of conversations that development teams should have around building event-driven or other microservice based architectures. These topics will help teams determine which architectural patterns are best for them, and assist in building their systems and processes in a reliable and supportable way.&lt;/p&gt;
&lt;p&gt;The &amp;quot;Critical C&apos;s&amp;quot; are: &lt;strong&gt;Context&lt;/strong&gt;, &lt;strong&gt;Consistency&lt;/strong&gt;, &lt;strong&gt;Contract&lt;/strong&gt;, &lt;strong&gt;Chaos&lt;/strong&gt;, &lt;strong&gt;Competencies&lt;/strong&gt; and &lt;strong&gt;Coalescence&lt;/strong&gt;. Each of these topics will be covered in detail in this series of articles. The first article of the 6 was on the subject of &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-1of6-context.html&quot;&gt;Context&lt;/a&gt;. This is article 4 of the series, and covers the topic of &lt;strong&gt;Chaos&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;Chaos&lt;/h4&gt;
&lt;p&gt;One of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing&quot;&gt;Fallacies of Distributed Computing&lt;/a&gt; is that the network is reliable. We should have similarly low expectations for the reliability of all of the infrastructure on which our services depend. Networks will segment, commodity servers and drives will fail, containers and operating systems will become unstable. In other words, our software will have errors during operation, no matter how resilient we attempt to make it. We need to embrace the fact that failures will occur in our software, and will do so at random times and often in unpredictable ways.&lt;/p&gt;
&lt;p&gt;If we are to build systems that don&apos;t require our constant attention, especially during off-hours, we need to be able to identify what happens when failures occur, and design our systems in ways that will allow them to heal automatically once the problem is corrected.&lt;/p&gt;
&lt;p&gt;To start this process, I recommend playing &amp;quot;what-if&amp;quot; games using diagrams of the system. Walk through the components of the system, and how the data flows through it, identifying each place where a failure could occur. Then, in each area where failures could happen, attempt to define the possible failure modes and explore what the impact of those failures might be. This kind of &amp;quot;virtual&amp;quot; Chaos Engineering is certainly no substitute for actual experimentation and testing, but is a good starting point for more in-depth analysis. It also can be very valuable in helping to understand the system and to produce more hardened services in the future.&lt;/p&gt;
&lt;p&gt;Thought experiments are useful, but you cannot really know how a system will respond to different types of failures until you have those failures in production. Historically, such &amp;quot;tests&amp;quot; have occurred at random, at the whim of the infrastructure, and usually at the worst possible time. Instead of leaving these things to chance, tools like &lt;em&gt;Chaos Monkey&lt;/em&gt; can be used to simulate failures in production, and can be configured to create these failures during times where the appropriate support engineers are available and ready to respond if necessary. This way, we can see if our systems respond as we expect, and more importantly, heal themselves as we expect.&lt;/p&gt;
&lt;p&gt;Even if you&apos;re not ready to jump into using automated experimentation tools in production just yet, a lot can be learned from using feature-flags and changing service behaviors in a more controlled manner as a starting point. This might involve a flag that can be set to cause an API method to return an error response, either as a hard failure, or during random requests for a period of time. Perhaps a switch could be set to stop a service from picking-up asynchronous messages from a queue or topic. Of course, these flags can only be placed in code we control, so we can&apos;t test failures of dependencies like databases and other infrastructure components in this way. For that, we&apos;ll need more involved testing methods.&lt;/p&gt;
&lt;p&gt;Regardless of how we test our systems, it is important that we do everything we can to build systems that will heal themselves without the need for us to intervene every time a failure occurs. As a result, I highly recommend using asynchronous messaging patterns whenever possible. The asynchrony of these tools allow our services to be &amp;quot;temporally decoupled&amp;quot; from their dependencies. As a result, if a container fails and is restarted by Kubernetes, any message in process is rolled-back onto the queue or topic, and the system can pick right up where it left off.&lt;/p&gt;
&lt;h4&gt;Goals of the Conversation&lt;/h4&gt;
&lt;p&gt;Development teams should have conversations around &lt;strong&gt;Chaos&lt;/strong&gt; that are primarily focused around procedures for identifying and remediating possible failure points in the application. These conversations should include answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How will we evaluate potential sources of failures in our systems before they are built?
&lt;ul&gt;
&lt;li&gt;How will we handle the inability to reach a dependency such as a database?&lt;/li&gt;
&lt;li&gt;How will we handle duplicate messages sent from our upstream data sources?&lt;/li&gt;
&lt;li&gt;How will we handle messages sent out-of-order from our upstream data sources?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How will we expose possible sources of failures during any pre-deployment testing?&lt;/li&gt;
&lt;li&gt;How will we expose possible sources of failures in the production environment before they occur for users?&lt;/li&gt;
&lt;li&gt;How will we identify errors that occur for users within production?&lt;/li&gt;
&lt;li&gt;How will we prioritize changes to the system based on the results of these experiments?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Next Up - Competencies&lt;/h4&gt;
&lt;p&gt;In the next article of this series we will look at &lt;a href=&quot;https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-5of6-competencies.html&quot;&gt;Competencies&lt;/a&gt; and how we should focus at least as much on what we build as how we build it.&lt;/p&gt;
</description>
  <link>https://www.cognitiveinheritance.com/Posts/critical-cs-of-microservices-4of6-chaos.html</link>
  <author>aa88801d-9543-467e-9d7e-8768d2d14aa7@bsstahl.com (Barry S. Stahl)</author>
  <guid>https://www.cognitiveinheritance.com/Permalinks/0d794105-ef08-4465-b806-4a8eff7523c7.html</guid>
  <pubDate>Mon, 02 Jan 2023 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>