---
author: bsstahl
tags:
- development
- responsibility
- ai
- algorithms
- ethics
categories:
- Tools
menuorder: 0
id: cefd35ad-e6c3-466d-907d-f6301212a9bb
title: Conceptual Associations Without Understanding
description: Exploring the potential of LLMs to generate novel insights by identifying associations in high-dimensional data
teaser: Large Language Models (LLMs) like GPT can identify novel associations in high-dimensional data, even without human-like understanding. They can link disparate ideas across domains, potentially leading to innovative insights.
ispublished: false
showinlist: false
buildifnotpublished: true
publicationdate: 2023-09-01T07:00:00Z
lastmodificationdate: 2023-09-01T08:00:00Z
slug: conceptual-associations-without-understanding

---
As engineers, we often deal with increasing levels of abstraction and complexity in decision-making. Recent machine learning advances, especially semantic models, have increased this complexity, causing misunderstandings. Specifically, the ability of these models to generate ideas not directly in their training data, and whether these require human-like understanding. Let's look into Large Language Models (LLMs) like GPT and their ability to produce new ideas without human-like comprehension.

LLMs operate in complex data spaces beyond human cognitive capabilities. They don't "understand" concepts as we do but can find associations that humans might overlook. For instance, an LLM might associate 'quantum entanglement' from physics with 'real-time communication' in computer science, suggesting a new method for data transfer technologies. Or it might draw parallels between 'utilitarianism' and 'machine learning optimization', resulting in new ethical frameworks for AI.

Let's examine how LLMs encode complex concepts like idioms and sarcasm. Take "Well, look who's on time". In the embeddings space, this phrase is closer to "Actually Late" than "Actually Early", indicating the sarcastic nature is part of the encoded value.

This example shows how LLMs capture nuanced meanings based on context. While they don't "understand" text as humans do, they can create context-rich text. However, it's crucial to remember that insights depend on the data. If biased or incomplete, the insights might be misleading. Human experts must interpret and apply these ideas within their domain.

In conclusion, while LLMs don't "understand" like humans, they can produce new ideas by identifying patterns in data. However, they are tools to support human decision-making, and their outputs need careful interpretation by experts.
